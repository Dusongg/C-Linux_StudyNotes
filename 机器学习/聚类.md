![image-20240603170153765](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603170153765.png)

无监督问题无法计算TP, FP， TN, FN



# k均值和k近邻的区别

K均值（K-means）和K近邻（K-nearest neighbors, KNN）是两种常见的机器学习算法，它们在应用和原理上有显著的区别。以下是它们的详细对比：

### K均值（K-means）算法

**定义**：K均值是一种无监督学习算法，主要用于聚类问题。它通过将数据分成K个簇，使得每个簇中的样本彼此之间的相似度最大。

**主要步骤**：
1. **初始化**：随机选择K个初始质心（簇的中心点）。
2. **分配**：将每个数据点分配给最近的质心，形成K个簇。
3. **更新**：计算每个簇的质心，更新质心位置为簇中所有点的平均值。
4. **重复**：重复分配和更新步骤，直到质心位置不再改变或达到预定的迭代次数。

**使用场景**：
- 图像压缩
- 文本分类
- 数据预处理（如降维）

**优缺点**：
- **优点**：
  - 简单易实现
  - 计算效率高
  - 对大数据集效果较好

- **缺点**：
  - 需要预先指定K值
  - 对初始质心敏感，可能陷入局部最优
  - 只适用于线性可分的数据

### K近邻（K-nearest neighbors, KNN）算法

**定义**：K近邻是一种监督学习算法，用于分类和回归问题。它通过计算与输入样本距离最近的K个训练样本，利用这些邻居的信息进行预测。

**主要步骤**：
1. **计算距离**：对给定的输入样本，计算它与所有训练样本之间的距离（如欧氏距离、曼哈顿距离）。
2. **选择邻居**：选择距离最近的K个训练样本。
3. **投票或平均**：对于分类问题，采用多数投票法决定输入样本的类别；对于回归问题，计算邻居的平均值作为预测值。

**使用场景**：
- 图像识别
- 文本分类
- 推荐系统

**优缺点**：
- **优点**：
  - 简单易实现
  - 无需训练过程，适用于小数据集
  - 可以处理多分类问题

- **缺点**：
  - 计算复杂度高，特别是对大数据集
  - 存储复杂度高，需要存储所有训练数据
  - 对噪声和不平衡数据敏感
  - 选择适当的K值和距离度量是关键

### 对比总结

| 特性           | K均值 (K-means)                       | K近邻 (KNN)                             |
| -------------- | ------------------------------------- | --------------------------------------- |
| **类型**       | 无监督学习（聚类）                    | 监督学习（分类和回归）                  |
| **目标**       | 将数据分成K个簇                       | 预测输入样本的类别或数值                |
| **输入**       | 需要输入K值                           | 需要输入K值                             |
| **计算复杂度** | 低，O(n \* K \* t)，t为迭代次数       | 高，O(n \* d)，d为特征维度              |
| **存储复杂度** | 低，只需存储K个质心                   | 高，需要存储所有训练数据                |
| **优点**       | 简单、快速、适用于大数据集            | 简单、直观、无需训练过程                |
| **缺点**       | 需预定义K值，对初始质心敏感，线性可分 | 计算和存储复杂度高，对K值和距离度量敏感 |
| **应用场景**   | 图像压缩、文本分类、数据预处理        | 图像识别、文本分类、推荐系统            |

K均值和K近邻算法各有优势，适用于不同的任务和数据特征。在选择算法时，应根据具体的应用场景、数据规模和计算资源等因素进行综合考虑。



## K-MEANS算法

### 基本概念

![image-20240603171230741](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603171230741.png)

### 工作流程

可视化网站：https://www.naftaliharris.com/blog/visualizing-k-means-clustering/

1. k = 2，则随机初始化两个点（初始化的点对最后分类结果影响较大），通过欧氏距离分成两个类

2. 更新质心

3. 重新分配，到第二步

   ![image-20240604095904250](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240604095904250.png)**

### 优缺点

![image-20240603171731614](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603171731614.png)

### 评估标准

1. inertia

中心点到该类其他点的距离总和

1. 轮廓系数

![image-20240603190304674](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603190304674.png)

![image-20240603190826759](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603190826759.png)

### 如何找到合适的k

![image-20240603185801826](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603185801826.png)

 随着k值的增大，inertia值会减小，可以找一个拐点作为k值

## DBSCAN算法

### 基本概念

![image-20240603172841631](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603172841631.png)

- 阈值minPts和半径r难选

![image-20240603173417143](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603173417143.png)

![image-20240603173537025](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603173537025.png)

### 工作流程

可视化网站：https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/

1. 输入参数：数据集，半径，密度阈值
2. 类似于BFS

![image-20240603173956437](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603173956437.png)

### 优缺点

![image-20240603174159909](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240603174159909.png)



### 



# 层次聚类

两种：分离聚类和。。





